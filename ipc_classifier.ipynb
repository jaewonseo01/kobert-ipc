{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8ecf4214c5e97f49"
  },
  {
   "cell_type": "markdown",
   "id": "46edd0a84894e434",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "KoBERT를 활용한 IPC 코드 예측기\n",
    "==================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad88b3532ad14a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Setup\n",
    "--------\n",
    "\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "976dc9064a7d3c51",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:07:17.203146800Z",
     "start_time": "2023-11-30T20:07:12.078751400Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings  # ignore warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "import datetime\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tqdm.auto import tqdm # notebook 사용시 적용\n",
    "\n",
    "\n",
    "from KoBERT.kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from KoBERT.kobert.utils import get_tokenizer\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e9940e389636b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "994bb69f37ca5bdc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:07:17.217159900Z",
     "start_time": "2023-11-30T20:07:17.207149800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Validation loss에 따른 조기종료 설정\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \n",
    "        # val_loss가 낮을 수록 좋은 모델 = 음수를 곱해 score로 사용 (score는 높을 수록 좋음)\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        # validation loss가 감소하면 체크포인트 저장\n",
    "        if self.verbose:\n",
    "            self.trace_func(\n",
    "                f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94b9ac5eaa1a20a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:07:17.240181800Z",
     "start_time": "2023-11-30T20:07:17.220163200Z"
    }
   },
   "outputs": [],
   "source": [
    "# BERT가 쓸 수 있는 데이터셋으로 바꿔주는 함수\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [i[label_idx] for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i],))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88c49cfa1f773e83",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:07:17.249188200Z",
     "start_time": "2023-11-30T20:07:17.238179100Z"
    }
   },
   "outputs": [],
   "source": [
    "# KoBERT 예제 코드\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size=768,\n",
    "                 num_classes=85,  # 데이터의 클래스 수로 조정\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(),\n",
    "                              attention_mask=attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        else:\n",
    "            out = pooler\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d6707de2f2babf7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:07:17.269208300Z",
     "start_time": "2023-11-30T20:07:17.251189800Z"
    }
   },
   "outputs": [],
   "source": [
    "class BalancedFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(BalancedFocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Calculate BCE loss\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "\n",
    "        # Calculate focal weights\n",
    "        focal_weights = torch.where(targets == 1, self.alpha * (1 - torch.sigmoid(logits))**self.gamma, (1 - self.alpha) * torch.sigmoid(logits)**self.gamma)\n",
    "\n",
    "        # Apply focal weights to BCE loss\n",
    "        balanced_focal_loss = focal_weights * bce_loss\n",
    "\n",
    "        # Apply reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return balanced_focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return balanced_focal_loss.sum()\n",
    "        else:\n",
    "            return balanced_focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f51ccdb397f0e7f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ecc95d2802520f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:07:17.282220Z",
     "start_time": "2023-11-30T20:07:17.267205500Z"
    }
   },
   "outputs": [],
   "source": [
    "# 길이, 단어 수 등 계산\n",
    "def calculate_length_stats(column):\n",
    "    max_len = column.str.len().max()\n",
    "    min_len = column.str.len().min()\n",
    "    avg_len = column.str.len().mean()\n",
    "    max_words = column.str.split().apply(len).max()\n",
    "    min_words = column.str.split().apply(len).min()\n",
    "    avg_words = column.str.split().apply(len).mean()\n",
    "    return max_len, min_len, avg_len, max_words, min_words, avg_words\n",
    "\n",
    "\n",
    "# logit을 0과 1 사이 확률 값으로 변환해주는 시그모이드\n",
    "def sigmoid(arr):\n",
    "    result = 1 / (1 + np.exp(-arr))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "seed_everything(42)  # Seed 고정"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:07:17.330301400Z",
     "start_time": "2023-11-30T20:07:17.316288100Z"
    }
   },
   "id": "e6d1389515734867"
  },
  {
   "cell_type": "markdown",
   "id": "4ea772825b62e101",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Preprocess\n",
    "-----\n",
    "\n",
    "### Import and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b44d2324ee6449d5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:07:36.099874100Z",
     "start_time": "2023-11-30T20:07:17.334304700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2023-12-01 05:07:17.333304\n",
      "================\n",
      "================\n",
      "\n",
      "GPU를 사용합니다.\n",
      "['B01D', 'B01F', 'B01J', 'B02C', 'B03C', 'B05B', 'B05C', 'B05D', 'B07B', 'B08B', 'B09B', 'B21B', 'B21C', 'B21D', 'B21J', 'B22D', 'B22F', 'B23B', 'B23C', 'B23D', 'B23K', 'B23P', 'B23Q', 'B24B', 'B24D', 'B25B', 'B25J', 'B26B', 'B26D', 'B28B', 'B29B', 'B29C', 'B29D', 'B29K', 'B29L', 'B30B', 'B31B', 'B32B', 'B33Y', 'B41F', 'B41J', 'B41M', 'B42D', 'B42F', 'B43K', 'B43L', 'B44C', 'B60B', 'B60C', 'B60G', 'B60H', 'B60J', 'B60K', 'B60L', 'B60N', 'B60P', 'B60Q', 'B60R', 'B60S', 'B60T', 'B60W', 'B61L', 'B62B', 'B62D', 'B62J', 'B62K', 'B62M', 'B63B', 'B63C', 'B63H', 'B63J', 'B64C', 'B64D', 'B65B', 'B65D', 'B65F', 'B65G', 'B65H', 'B66B', 'B66C', 'B66D', 'B66F', 'B67D', 'B82B', 'B82Y']\n",
      "\n",
      "================\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 172412 entries, 0 to 172411\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   초록      172412 non-null  object\n",
      " 1   ipc코드   172412 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.6+ MB\n",
      "None\n",
      "\n",
      "================\n",
      "\n",
      "                                                  초록  \\\n",
      "0  상승 및 하강 그리고 수평이동에스컬레이터의 자동운전 장치에 관한 것으로 특히 에스컬...   \n",
      "1  폐비닐과 제지슬러지 등 폐자재를 이용하여 재생 가능한 재활용품을 제조하기 위한 방법...   \n",
      "2  나노입자 어레이 제조방법이에 의하여 제조된 나노입자 어레이플라즈몬나노입자를 포함하는...   \n",
      "3  발명은 일측의 아이들 보빈에 감겨 있는 스테인레스 등의 선재를 타 측의 구동 드럼으...   \n",
      "4  본 발명에 따른 폐타이어 분말을 제조하기 위하여 우선 폐타이어를 절단하여 폐타이어 ...   \n",
      "\n",
      "                                               ipc코드  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "\n",
      "================\n",
      "\n",
      "data example : 발명은 일측의 아이들 보빈에 감겨 있는 스테인레스 등의 선재를 타 측의 구동 드럼으로 당겨내면서 아이들 보빈과 구동 드럼 사이에 압연롤을 설치하여 선재를 일정직경의 선재로 늘려나가는 압연롤에 의한 선재의 신선가공장치에 관한 것이 다 발명은 일측에 위치하여 원주면 상에선 재가 감겨있으며 제자리에서 공회전을 하는 아이들 보빈과 상기 아이들 보빈에서 풀려나오는 선재를 압연을 하여 단면의 직경을 축소시키는 압연롤과 상기압 연롤에서 압연이 되어 나오는 선재에 구동력을  주며 감아주는 구동 드럼을 포함하는 것이 다 발명에 의하면 압연롤에 압연유를 사용하여 습식으로 신선가공을 하면서도 여러 두 이상의 압연롤과 구동 드럼을 연속적으로 설치하여 선재의 단면직경을 점차적으로 축소해 나갈 수 있으므로 신선가공의 생산성을 향상시킬 수 있으며 습식의 압연롤을 사용함으로써 표면조도도 향상시킬 수 있는 효과가 있다 한 본발명에 의하면 고가의 초경이나 다이아몬드 재질의 공구를 사용하는 인발 다이스 대신에 면압연롤로 압연을 하여 신선가공을 함으로써 공구의 수명을 연장시키고 공구비를 절감할 수 있는 효과가 있다 발 다이스선재압연롤\n",
      "\n",
      "================\n",
      "\n",
      "<class 'list'>\n",
      "\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "print(f'Starting at {datetime.datetime.now()}')  # 시작 시간\n",
    "print('================')\n",
    "print('================\\n')\n",
    "# check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"GPU를 사용합니다.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CPU를 사용합니다.\")\n",
    "\n",
    "# 데이터 불러오기\n",
    "data_a = pd.read_csv(\"pp_b.csv\")\n",
    "# 불러온 데이터의 ipc코드 column의 값들을 list로 변경경\n",
    "data_a['ipc코드'] = data_a['ipc코드'].apply(ast.literal_eval)\n",
    "\n",
    "# ipc코드 목록 불러오기\n",
    "ipc_file_name = \"ipc_b.txt\"\n",
    "ipc_a_dict = []\n",
    "with open(ipc_file_name, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Remove any leading/trailing whitespace and append the item to the list\n",
    "        item = line.strip()\n",
    "        ipc_a_dict.append(item)\n",
    "\n",
    "print(ipc_a_dict)  # 실제 ipc코드 확인\n",
    "print('\\n================\\n')\n",
    "\n",
    "print(data_a.info())  # 데이터 기본적인 정보 확인\n",
    "print('\\n================\\n')\n",
    "print(data_a.head())\n",
    "print('\\n================\\n')\n",
    "print(f'data example : {data_a.iloc[3, 0]}')  # 데이터 예시 확인\n",
    "print('\\n================\\n')\n",
    "print(type(data_a.iloc[3, 1]))  # 데이터 ipc코드 column이 list로 제대로 변환되었는지 확인\n",
    "print('\\n================\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2030eac25f1315a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Transform data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "994dcd075c41f7b4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:07:37.298560500Z",
     "start_time": "2023-11-30T20:07:36.103877200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train / val / test set : 84481 / 36207 / 51724\n",
      "\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "# data를 list로 변경, list의 element 하나가 ['초록', [ipc 코드 one-hot encoding된 list]]\n",
    "data_a_list = []\n",
    "\n",
    "for ques, label in zip(data_a['초록'], data_a['ipc코드']):\n",
    "    tmp_data = []  # 위에 언급한 'list의 element 하나'\n",
    "    tmp_data.append(ques)\n",
    "    tmp_data.append(np.array(label))\n",
    "    data_a_list.append(tmp_data)\n",
    "\n",
    "# train & test 데이터로 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train, test(검증용 데이터), test_final (최종 test용)\n",
    "a_traval, a_test_final = train_test_split(data_a_list, test_size=0.3, random_state=9871)\n",
    "a_train, a_test = train_test_split(a_traval, test_size=0.3, random_state=23)\n",
    "\n",
    "print(f'length of train / val / test set : {len(a_train)} / {len(a_test)} / {len(a_test_final)}')\n",
    "print('\\n================\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2297a3e29a74815b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define model\n",
    "-----\n",
    "\n",
    "### Define hyperparameters and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbaeda7c94ccad90",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:10:46.547912100Z",
     "start_time": "2023-11-30T20:07:37.298560500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. C:\\Users\\user\\Desktop\\capstone\\.cache\\kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
      "using cached model. C:\\Users\\user\\Desktop\\capstone\\.cache\\kobert_v1.zip\n",
      "using cached model. C:\\Users\\user\\Desktop\\capstone\\.cache\\kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter\n",
    "'''\n",
    "max_len : BERT 모델이 한 번에 받는 최대 word token의 수 (최대 512)\n",
    "batch_size : 한 번에 처리할 batch의 size\n",
    "warmup_ratio : 학습률이 유동적으로 변하게 해주느 Learning rate scheduler의 hyperparameter https://ai4nlp.tistory.com/8\n",
    "num_epochs : 총 학습할 epoch\n",
    "max_grad_norm : 오차 역전파 과정에서 일어날 수 있는 학습 무효화 방지 https://eehoeskrap.tistory.com/582\n",
    "log_interval : 몇 batch마다 accuracy 보여줄지(학습과는 상관 X, 확인용)\n",
    "learning_rate : 학습률, 현재 optimizer는 Adam으로 5e-5, 2e-5, 3e-5, 1e-4 정도가 해볼만 함 https://arxiv.org/pdf/1810.04805.pdf (BERT 공식 문서 appendix에 적혀있음)\n",
    "threshold : logit으로 반환된 예측 결과에 sigmoid를 적용해 확률 값으로 나타내는데, 확률 값이 threshold 이상이어야 해당 label인 것으로 판단\n",
    "patience : patience만큼의 epoch 동안 validation loss가 좋아지지 않으면 학습 종료\n",
    "'''\n",
    "max_len = 512\n",
    "batch_size = 8\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 15\n",
    "max_grad_norm = 1 \n",
    "log_interval = 10000 \n",
    "learning_rate = 5e-5 \n",
    "threshold = 0.5  \n",
    "patience = 5\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "# BERT 모델, Vocabulary 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "# BERTDataset : 각 데이터가 BERT 모델의 입력으로 들어갈 수 있도록 tokenization, int encoding, padding하는 함수\n",
    "data_train = BERTDataset(a_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(a_test, 0, 1, tok, max_len, True, False)\n",
    "data_test_final = BERTDataset(a_test_final, 0, 1, tok, max_len, True, False)\n",
    "\n",
    "# torch 형식의 dataset을 만들어 입력 데이터셋의 전처리 마무리\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=0)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=0)\n",
    "final_dataloader = torch.utils.data.DataLoader(data_test_final, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276f256064be084",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load model, define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fd55a488e9d7786",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:10:47.052418600Z",
     "start_time": "2023-11-30T20:10:46.550915100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10561\n"
     ]
    }
   ],
   "source": [
    "# BERT 모델 불러오기\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "\n",
    "# optimizer와 schedule 설정\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "# loss_fn = nn.CrossEntropyLoss() # 다중분류를 위한 loss function\n",
    "loss_fn = BalancedFocalLoss(alpha=0.05, gamma=2, reduction='mean')\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs  # 전체 시행하게 될 학습 횟수\n",
    "warmup_step = int(t_total * warmup_ratio)  # 전체 학습의 warmup_ratio 비율만큼 warmup 단계로 설정\n",
    "print(len(train_dataloader))\n",
    "\n",
    "# warmup 단계에 필요한 scheduler\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ad7a8cef5b033",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Model training\n",
    "--------\n",
    "\n",
    "### Train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "974d8515ecb010e1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:45:13.365695300Z",
     "start_time": "2023-11-30T20:10:47.063428600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10561 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b01076e982b4453aa47cdb91a943805"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch number 0, train label-wise accuracy is : 0.0\n",
      "epoch 0, batch number 10000, train label-wise accuracy is : 0.0\n",
      "=====Training Report: Epoch 0 mean loss is 0.013499485328793526=====\n",
      "Accuracy is 0.00010653282986707071\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4526 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd4af980e264408c83c876169eca839e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Validation Report: mean loss is 0.0014008455909788609=====\n",
      "Accuracy is 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10561 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd053a27828649ba96ffd91c411ceea9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch number 0, train label-wise accuracy is : 0.0\n",
      "epoch 1, batch number 10000, train label-wise accuracy is : 0.0\n",
      "=====Training Report: Epoch 1 mean loss is 0.0011442877585068345=====\n",
      "Accuracy is 0.032587208958227296\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4526 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad6b96b1a1f840cf94a6535231db5b1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Validation Report: mean loss is 0.00091611931566149=====\n",
      "Accuracy is 0.06653409561686967\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10561 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2b5b939a3a8495e8ae5b6c4a1d43516"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, batch number 0, train label-wise accuracy is : 0.0\n",
      "epoch 2, batch number 10000, train label-wise accuracy is : 0.0\n",
      "=====Training Report: Epoch 2 mean loss is 0.0009010476060211658=====\n",
      "Accuracy is 0.1265846758442727\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4526 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c902b7623284e62a7a43c3d4cc75695"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Validation Report: mean loss is 0.0008285867515951395=====\n",
      "Accuracy is 0.18170519512801392\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10561 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ee577d71efd444ebd88930a685947d9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch number 0, train label-wise accuracy is : 0.0\n",
      "epoch 3, batch number 10000, train label-wise accuracy is : 0.125\n",
      "=====Training Report: Epoch 3 mean loss is 0.0007951883017085493=====\n",
      "Accuracy is 0.2016784839194612\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4526 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97d2c035e9be474b8daf55911d8621c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Validation Report: mean loss is 0.0008156502735801041=====\n",
      "Accuracy is 0.22382412240726932\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10561 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b90d828d530c447ead0ee71156df3b13"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, batch number 0, train label-wise accuracy is : 0.125\n",
      "epoch 4, batch number 10000, train label-wise accuracy is : 0.25\n",
      "=====Training Report: Epoch 4 mean loss is 0.0007197097293101251=====\n",
      "Accuracy is 0.2684745682461145\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4526 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc975048c2d44073aaa357548a31617b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Validation Report: mean loss is 0.0008331857970915735=====\n",
      "Accuracy is 0.3419780705388461\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10561 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec6d40386a61494788f304e8f861f906"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, batch number 0, train label-wise accuracy is : 0.5\n",
      "epoch 5, batch number 10000, train label-wise accuracy is : 0.375\n",
      "=====Training Report: Epoch 5 mean loss is 0.0006451467634178698=====\n",
      "Accuracy is 0.3412956759508055\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4526 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e6b2c6eaa454ca198b47f8f9abba092"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Validation Report: mean loss is 0.0008461812394671142=====\n",
      "Accuracy is 0.3602618278233491\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10561 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "618c3a8e033343ea9200f4630ab35ab6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, batch number 0, train label-wise accuracy is : 0.125\n",
      "epoch 6, batch number 10000, train label-wise accuracy is : 0.5\n",
      "=====Training Report: Epoch 6 mean loss is 0.0005737622268497944=====\n",
      "Accuracy is 0.4172772576082196\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4526 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9edcc15e10b405f9f4be1740cfa2f48"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Validation Report: mean loss is 0.0008938690880313516=====\n",
      "Accuracy is 0.40351313281962053\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10561 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08bb5c4c6581494fafec563d66e76554"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, batch number 0, train label-wise accuracy is : 0.25\n",
      "epoch 7, batch number 10000, train label-wise accuracy is : 0.375\n",
      "=====Training Report: Epoch 7 mean loss is 0.0005002529360353947=====\n",
      "Accuracy is 0.4969283034054995\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4526 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3e890a75e014d4fa88fba92b4157802"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Validation Report: mean loss is 0.0009259448270313442=====\n",
      "Accuracy is 0.4124892976496258\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10561 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b3adf0534d247f4acd9244d62b70ba7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, batch number 0, train label-wise accuracy is : 0.375\n",
      "epoch 8, batch number 10000, train label-wise accuracy is : 0.625\n",
      "=====Training Report: Epoch 8 mean loss is 0.00043250719318166375=====\n",
      "Accuracy is 0.5698559439400576\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4526 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d44336b73b674ee6881993200a3a9c63"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Validation Report: mean loss is 0.0010797528084367514=====\n",
      "Accuracy is 0.49636810561493633\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10561 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be644dc1fcf845a1b8d47c4c0cf8fbb9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, batch number 0, train label-wise accuracy is : 0.5\n",
      "epoch 9, batch number 10000, train label-wise accuracy is : 0.75\n",
      "=====Training Report: Epoch 9 mean loss is 0.00036864852881990373=====\n",
      "Accuracy is 0.6378238893952486\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4526 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5cc19bc655644388976d76f6fde43a59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Validation Report: mean loss is 0.0012021156726405025=====\n",
      "Accuracy is 0.5243737398845527\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10561 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2081c58cf7e44e31b3e3bcaa18ea94bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, batch number 0, train label-wise accuracy is : 0.75\n",
      "epoch 10, batch number 10000, train label-wise accuracy is : 0.75\n",
      "=====Training Report: Epoch 10 mean loss is 0.0003089872479904443=====\n",
      "Accuracy is 0.7013056190149264\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4526 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "378473025e0441f392f83c64a046430a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Validation Report: mean loss is 0.001393481157720089=====\n",
      "Accuracy is 0.5625155356698981\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10561 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea38bab1d99e46709e1ece1ce2e090b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, batch number 0, train label-wise accuracy is : 0.875\n",
      "epoch 11, batch number 10000, train label-wise accuracy is : 0.625\n",
      "=====Training Report: Epoch 11 mean loss is 0.00025905465008690953=====\n",
      "Accuracy is 0.7496123388690948\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4526 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9a8193e225a4a6e850072f7193af7e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Validation Report: mean loss is 0.0014563982840627432=====\n",
      "Accuracy is 0.5751097854006132\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10561 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22b19e125d9643f4a9b214f01678ebda"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, batch number 0, train label-wise accuracy is : 0.75\n",
      "epoch 12, batch number 10000, train label-wise accuracy is : 0.625\n",
      "=====Training Report: Epoch 12 mean loss is 0.0002215589047409594=====\n",
      "Accuracy is 0.7861649364945964\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4526 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b8b4b5722af42b98e29de038c12dda6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Validation Report: mean loss is 0.0016046615783125162=====\n",
      "Accuracy is 0.5912116441572072\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10561 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef57e1cc00ba4d47926b9018f4b0cf65"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, batch number 0, train label-wise accuracy is : 1.0\n",
      "epoch 13, batch number 10000, train label-wise accuracy is : 0.75\n",
      "=====Training Report: Epoch 13 mean loss is 0.0001989662559935823=====\n",
      "Accuracy is 0.8067967945455191\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4526 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1aa32568b8004e73b08cafe03b0e0b18"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Validation Report: mean loss is 0.001681844936683774=====\n",
      "Accuracy is 0.5958792498688099\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10561 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0927cf7766e64580a14ed78a5e32eab3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, batch number 0, train label-wise accuracy is : 1.0\n",
      "epoch 14, batch number 10000, train label-wise accuracy is : 0.75\n",
      "=====Training Report: Epoch 14 mean loss is 0.0001902539370348677=====\n",
      "Accuracy is 0.8146447130123933\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4526 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14e354cdc0964f2084327378c780dd8a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Validation Report: mean loss is 0.0016979699721559882=====\n",
      "Accuracy is 0.5994144778633966\n",
      "train acc : [0.00010653282986707071, 0.032587208958227296, 0.1265846758442727, 0.2016784839194612, 0.2684745682461145, 0.3412956759508055, 0.4172772576082196, 0.4969283034054995, 0.5698559439400576, 0.6378238893952486, 0.7013056190149264, 0.7496123388690948, 0.7861649364945964, 0.8067967945455191, 0.8146447130123933]\n",
      "test acc : [0.0, 0.06653409561686967, 0.18170519512801392, 0.22382412240726932, 0.3419780705388461, 0.3602618278233491, 0.40351313281962053, 0.4124892976496258, 0.49636810561493633, 0.5243737398845527, 0.5625155356698981, 0.5751097854006132, 0.5912116441572072, 0.5958792498688099, 0.5994144778633966]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_history = []\n",
    "test_history = []\n",
    "loss_history = []\n",
    "\n",
    "# epoch 당 평균 loss 기록 (training)\n",
    "avg_train_losses = []\n",
    "# epoch 당 평균 loss 기록 (validation)\n",
    "avg_valid_losses = []\n",
    "\n",
    "es_count = 0\n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True, path='checkpoint.pt')\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    count_acc_tr = 0\n",
    "    count_total_tr = 0\n",
    "    count_acc_test = 0\n",
    "    count_total_test = 0\n",
    "    model.train()\n",
    "    train_epoch_pred = []\n",
    "    train_loss_record = []\n",
    "    # epoch 내에서 training loss 기록\n",
    "    train_losses = []\n",
    "    # epoch 내에서 validation loss 기록\n",
    "    valid_losses = []\n",
    "\n",
    "\n",
    "    # Train\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(train_dataloader),\n",
    "                                                                        total=len(train_dataloader)):\n",
    "        optimizer.zero_grad() # optimizer의 gradient 0으로 초기화 \n",
    "\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length = valid_length\n",
    "\n",
    "        label = label.float().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "\n",
    "        train_loss_record.append(loss)\n",
    "\n",
    "        train_pred = out.detach().cpu().numpy()\n",
    "        train_pred = sigmoid(train_pred)  # logit에 sigmoid 적용해서 0과 1 사이(확률)로 변환\n",
    "        train_pred = (train_pred >= threshold).astype(float)  # threshold 기준으로 label 판단\n",
    "        train_real = label.detach().cpu().numpy()\n",
    "\n",
    "        if batch_id % log_interval == 0:\n",
    "            train_batch_result = accuracy_score(np.array(train_real), np.array(train_pred))\n",
    "            print(\n",
    "                f\"epoch {e}, batch number {batch_id}, train label-wise accuracy is : {train_batch_result}\")\n",
    "        train_epoch_pred.append(train_pred)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    train_epoch_pred = np.concatenate(train_epoch_pred)\n",
    "    train_epoch_target = train_dataloader.dataset.labels\n",
    "    train_epoch_result = accuracy_score(y_true=train_epoch_target, y_pred=train_epoch_pred)\n",
    "\n",
    "\n",
    "    print(f\"=====Training Report: Epoch {e} mean loss is {sum(train_loss_record) / len(train_loss_record)}=====\")\n",
    "    print(f'Accuracy is {train_epoch_result}')\n",
    "    model.eval() # 학습 모드가 아닌 예측 모드로 변경\n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad(): # gradient 계산 X로 추론 속도 빨라짐\n",
    "        test_epoch_pred = []\n",
    "        test_loss_record = []\n",
    "        test_acc = 0.0\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(test_dataloader),\n",
    "                                                                            total=len(test_dataloader)):\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length = valid_length\n",
    "\n",
    "            label = label.float().to(device)\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            loss = loss_fn(out, label)\n",
    "\n",
    "            test_loss_record.append(loss)\n",
    "\n",
    "            test_pred = out.detach().cpu().numpy()\n",
    "            test_pred = sigmoid(test_pred)\n",
    "            test_pred = (test_pred >= threshold).astype(float)\n",
    "            test_real = label.detach().cpu().numpy()\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "\n",
    "            test_batch_result = accuracy_score(np.array(test_real), np.array(test_pred))\n",
    "\n",
    "            test_epoch_pred.append(test_pred)\n",
    "\n",
    "        test_epoch_pred = np.concatenate(test_epoch_pred)\n",
    "        test_epoch_target = test_dataloader.dataset.labels\n",
    "        test_epoch_result = accuracy_score(y_true=test_epoch_target, y_pred=test_epoch_pred)\n",
    "\n",
    "\n",
    "        print(f\"=====Validation Report: mean loss is {sum(test_loss_record) / len(test_loss_record)}=====\")\n",
    "        print(f'Accuracy is {test_epoch_result}')\n",
    "\n",
    "\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        train_loss = np.average(train_losses)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        \n",
    "        train_history.append(train_epoch_result)\n",
    "        test_history.append(test_epoch_result)\n",
    "        \n",
    "        if e > 0:\n",
    "            if test_history[e] < test_history[e-1]:\n",
    "                es_count += 1\n",
    "            else:\n",
    "                es_count = 0\n",
    "                torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        if es_count == 3:\n",
    "            break\n",
    "\n",
    "        # 조기 종료 체크\n",
    "        \"\"\"\n",
    "        early_stopping(valid_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "# 시각화\n",
    "plt.plot(avg_train_losses, label='Train Loss')\n",
    "plt.plot(avg_valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss_a.png')\n",
    "plt.close()\n",
    "\n",
    "print(f'train acc : {train_history}')\n",
    "print(f'test acc : {test_history}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73044bd4c0b69167",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Test model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b78a7bb3df22f45",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:55:26.026959700Z",
     "start_time": "2023-12-01T10:45:13.372700700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/6466 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a66afac2a3a74156950a0514fe9bf32b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Final Testing Report: mean loss is 0.0016765096224844456=====\n",
      "Accuracy is 0.5968022581393551\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "torch.save(model.state_dict(), 'bert_b.pth')\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    final_epoch_pred = []\n",
    "    final_loss_record = []\n",
    "    # test done per epoch\n",
    "    final_test_acc = 0.0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(final_dataloader),\n",
    "                                                                        total=len(final_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length = valid_length\n",
    "\n",
    "        label = label.float().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "\n",
    "        final_loss_record.append(loss)\n",
    "\n",
    "        f_pred = out.detach().cpu().numpy()\n",
    "        f_pred = sigmoid(f_pred)\n",
    "        f_pred = (f_pred >= threshold).astype(float)\n",
    "        f_real = label.detach().cpu().numpy()\n",
    "\n",
    "        final_batch_result = accuracy_score(np.array(f_real), np.array(f_pred))\n",
    "\n",
    "        final_epoch_pred.append(f_pred)\n",
    "\n",
    "    final_epoch_pred = np.concatenate(final_epoch_pred)\n",
    "    final_epoch_target = final_dataloader.dataset.labels\n",
    "    final_epoch_result = accuracy_score(y_true=final_epoch_target, y_pred=final_epoch_pred)\n",
    "\n",
    "\n",
    "    print(f\"=====Final Testing Report: mean loss is {sum(final_loss_record) / len(final_loss_record)}=====\")\n",
    "    print(f'Accuracy is {final_epoch_result}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
